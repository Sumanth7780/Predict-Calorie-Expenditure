{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14890fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated RMSLE (mean): 0.06137\n",
      "Fold-wise RMSLE: [0.06144635 0.06176392 0.06119345 0.06129091 0.06116859]\n",
      "✅ Submission saved to: D:\\0-projects\\1- calorie prediction\\submission_cv_feateng.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "train_path = \"D:\\\\0-projects\\\\1- calorie prediction\\\\Data\\\\train.csv\"\n",
    "test_path = \"D:\\\\0-projects\\\\1- calorie prediction\\\\Data\\\\test.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Encode Sex column\n",
    "le = LabelEncoder()\n",
    "train_df['Sex'] = le.fit_transform(train_df['Sex'])\n",
    "test_df['Sex'] = le.transform(test_df['Sex'])\n",
    "\n",
    "# Feature Engineering\n",
    "def add_features(df):\n",
    "    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "    df['Duration_x_HR'] = df['Duration'] * df['Heart_Rate']\n",
    "    return df\n",
    "\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "# Features and target\n",
    "X = train_df.drop(columns=['id', 'Calories'])\n",
    "y = np.log1p(train_df['Calories'])  # log1p to avoid log(0)\n",
    "\n",
    "# Model\n",
    "model = XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "\n",
    "# Custom scorer for cross_val_score\n",
    "def rmsle_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "rmsle_cv = make_scorer(rmsle_scorer, greater_is_better=False)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf, scoring=rmsle_cv)\n",
    "print(f\"Cross-Validated RMSLE (mean): {-np.mean(cv_scores):.5f}\")\n",
    "print(f\"Fold-wise RMSLE: {-cv_scores}\")\n",
    "\n",
    "# Train final model on full data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.expm1(test_preds)  # Invert log1p\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Calories': test_preds\n",
    "})\n",
    "submission_file = \"D:\\\\0-projects\\\\1- calorie prediction\\\\submission_cv_feateng.csv\"\n",
    "submission.to_csv(submission_file, index=False)\n",
    "print(f\"✅ Submission saved to: {submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f7f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/5e/23/f8b28ca248bb629b9e08f877dd2965d1994e1674a03d67cd10c5246da248/lightgbm-4.6.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\suman\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\suman\\anaconda3\\lib\\site-packages (from lightgbm) (1.10.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.5 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4799a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/d9/dd/0b593d1a5ee431b33a1fdf4ddb5911c312ed3bb598ef9e17457af2ee7b34/optuna-4.3.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/31/59/565286efff3692c5716c212202af61466480f6357c4ae3089d4453bff1f3/alembic-1.16.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\suman\\anaconda3\\lib\\site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suman\\appdata\\roaming\\python\\python311\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\suman\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\suman\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\suman\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/87/fb/99f81ac72ae23375f22b7afdb7642aba97c00a713c217124420147681a2f/mako-1.3.10-py3-none-any.whl.metadata\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\suman\\appdata\\roaming\\python\\python311\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\suman\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\suman\\appdata\\roaming\\python\\python311\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\suman\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "   ---------------------------------------- 0.0/386.6 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 174.1/386.6 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 386.6/386.6 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 242.5/242.5 kB ? eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bd5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:23:58,438] A new study created in memory with name: no-name-698c9713-d4b2-4fce-9a37-d071843a9f25\n",
      "[I 2025-05-29 16:24:09,175] Trial 0 finished with value: 0.05994733463569154 and parameters: {'n_estimators': 250, 'max_depth': 10, 'learning_rate': 0.05302647493479643, 'subsample': 0.693744345176625, 'colsample_bytree': 0.8807753968651582}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:12,720] Trial 1 finished with value: 0.06187030351448925 and parameters: {'n_estimators': 194, 'max_depth': 4, 'learning_rate': 0.13985337903947376, 'subsample': 0.9393808692275227, 'colsample_bytree': 0.8313515385425629}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:18,169] Trial 2 finished with value: 0.0621593637143689 and parameters: {'n_estimators': 263, 'max_depth': 4, 'learning_rate': 0.05105581105185839, 'subsample': 0.7535740267797422, 'colsample_bytree': 0.8626057299689619}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:20,844] Trial 3 finished with value: 0.06340094810680938 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.15095618275603018, 'subsample': 0.502991159297193, 'colsample_bytree': 0.6303379716576756}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:25,313] Trial 4 finished with value: 0.06185916665801205 and parameters: {'n_estimators': 265, 'max_depth': 4, 'learning_rate': 0.2976089770644533, 'subsample': 0.6328471743655142, 'colsample_bytree': 0.542920350208659}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:27,906] Trial 5 finished with value: 0.06054088043068769 and parameters: {'n_estimators': 122, 'max_depth': 7, 'learning_rate': 0.10429902921559332, 'subsample': 0.6473933628877998, 'colsample_bytree': 0.9540718491950524}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:33,510] Trial 6 finished with value: 0.060760954265510564 and parameters: {'n_estimators': 287, 'max_depth': 5, 'learning_rate': 0.16408092408766353, 'subsample': 0.8960228946338444, 'colsample_bytree': 0.6031595300342381}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:36,063] Trial 7 finished with value: 0.061315921010237355 and parameters: {'n_estimators': 118, 'max_depth': 6, 'learning_rate': 0.22987394108162498, 'subsample': 0.7176799908533025, 'colsample_bytree': 0.7404217939705323}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:44,605] Trial 8 finished with value: 0.06246784848871617 and parameters: {'n_estimators': 238, 'max_depth': 10, 'learning_rate': 0.19398421598270582, 'subsample': 0.6381182594154519, 'colsample_bytree': 0.5790221421604873}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:50,990] Trial 9 finished with value: 0.060092433957226875 and parameters: {'n_estimators': 249, 'max_depth': 7, 'learning_rate': 0.09900413532893353, 'subsample': 0.8787947176451053, 'colsample_bytree': 0.7741201765577939}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:24:59,292] Trial 10 finished with value: 0.0772073411703933 and parameters: {'n_estimators': 198, 'max_depth': 10, 'learning_rate': 0.015179300592349315, 'subsample': 0.8088303622792634, 'colsample_bytree': 0.9782323522498592}. Best is trial 0 with value: 0.05994733463569154.\n",
      "[I 2025-05-29 16:25:06,139] Trial 11 finished with value: 0.059788122782424145 and parameters: {'n_estimators': 224, 'max_depth': 8, 'learning_rate': 0.0781372011217516, 'subsample': 0.8665046927494857, 'colsample_bytree': 0.7445037736138784}. Best is trial 11 with value: 0.059788122782424145.\n",
      "[I 2025-05-29 16:25:14,017] Trial 12 finished with value: 0.05973588328228004 and parameters: {'n_estimators': 223, 'max_depth': 9, 'learning_rate': 0.058263250287522786, 'subsample': 0.9811724692790761, 'colsample_bytree': 0.6953542031045485}. Best is trial 12 with value: 0.05973588328228004.\n",
      "[I 2025-05-29 16:25:18,785] Trial 13 finished with value: 0.059852435472125765 and parameters: {'n_estimators': 164, 'max_depth': 8, 'learning_rate': 0.08027083069205694, 'subsample': 0.9978227774850548, 'colsample_bytree': 0.6952232161607472}. Best is trial 12 with value: 0.05973588328228004.\n",
      "[I 2025-05-29 16:25:25,038] Trial 14 finished with value: 0.07172764380664609 and parameters: {'n_estimators': 221, 'max_depth': 8, 'learning_rate': 0.015311959894292203, 'subsample': 0.8341682505778303, 'colsample_bytree': 0.6724659512737017}. Best is trial 12 with value: 0.05973588328228004.\n",
      "[I 2025-05-29 16:25:31,235] Trial 15 finished with value: 0.0597112597227255 and parameters: {'n_estimators': 177, 'max_depth': 9, 'learning_rate': 0.06534313674452398, 'subsample': 0.9980898892796316, 'colsample_bytree': 0.7819861863654916}. Best is trial 15 with value: 0.0597112597227255.\n",
      "[I 2025-05-29 16:25:36,308] Trial 16 finished with value: 0.060081867431322 and parameters: {'n_estimators': 170, 'max_depth': 9, 'learning_rate': 0.12137507277106632, 'subsample': 0.9840278997320702, 'colsample_bytree': 0.7934423032107568}. Best is trial 15 with value: 0.0597112597227255.\n",
      "[I 2025-05-29 16:25:43,361] Trial 17 finished with value: 0.059679814643417685 and parameters: {'n_estimators': 173, 'max_depth': 9, 'learning_rate': 0.04882426604751229, 'subsample': 0.9288261932315841, 'colsample_bytree': 0.6753133616826141}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:25:48,695] Trial 18 finished with value: 0.06366765953021947 and parameters: {'n_estimators': 165, 'max_depth': 9, 'learning_rate': 0.03329212112463607, 'subsample': 0.9231219336376679, 'colsample_bytree': 0.5148142552961763}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:25:53,097] Trial 19 finished with value: 0.06148229326471792 and parameters: {'n_estimators': 182, 'max_depth': 6, 'learning_rate': 0.18899619675643872, 'subsample': 0.8113563381005291, 'colsample_bytree': 0.9187323299317679}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:25:59,096] Trial 20 finished with value: 0.05981625562539698 and parameters: {'n_estimators': 142, 'max_depth': 9, 'learning_rate': 0.08144395798524807, 'subsample': 0.9377304959428113, 'colsample_bytree': 0.6484448452430536}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:26:07,714] Trial 21 finished with value: 0.05973647492180057 and parameters: {'n_estimators': 219, 'max_depth': 9, 'learning_rate': 0.05117995986539471, 'subsample': 0.9991996678962689, 'colsample_bytree': 0.7099392701999945}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:26:12,383] Trial 22 finished with value: 0.05985732309561501 and parameters: {'n_estimators': 147, 'max_depth': 8, 'learning_rate': 0.060686698366225006, 'subsample': 0.9594111543106751, 'colsample_bytree': 0.7186842346349139}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:26:20,108] Trial 23 finished with value: 0.06042709193499367 and parameters: {'n_estimators': 203, 'max_depth': 10, 'learning_rate': 0.1110612782409757, 'subsample': 0.912120987580756, 'colsample_bytree': 0.7981490754265551}. Best is trial 17 with value: 0.059679814643417685.\n",
      "[I 2025-05-29 16:26:28,422] Trial 24 finished with value: 0.06053348188155821 and parameters: {'n_estimators': 189, 'max_depth': 9, 'learning_rate': 0.0311338734229513, 'subsample': 0.9680448809780909, 'colsample_bytree': 0.6326616785942063}. Best is trial 17 with value: 0.059679814643417685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Optuna: {'n_estimators': 173, 'max_depth': 9, 'learning_rate': 0.04882426604751229, 'subsample': 0.9288261932315841, 'colsample_bytree': 0.6753133616826141}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 870\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.141163\n",
      "Ensemble RMSLE on Validation Set: 0.05941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 869\n",
      "[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.141144\n",
      "✅ Final ensemble submission saved to: D:\\0-projects\\1- calorie prediction\\submission_ensemble_optuna.csv\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline with Optuna tuning, feature engineering, log-transform, and model ensembling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "\n",
    "# Load data\n",
    "train_path = \"D:\\\\0-projects\\\\1- calorie prediction\\\\Data\\\\train.csv\"\n",
    "test_path = \"D:\\\\0-projects\\\\1- calorie prediction\\\\Data\\\\test.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Encode 'Sex'\n",
    "le = LabelEncoder()\n",
    "train_df['Sex'] = le.fit_transform(train_df['Sex'])\n",
    "test_df['Sex'] = le.transform(test_df['Sex'])\n",
    "\n",
    "# Feature engineering\n",
    "def add_features(df):\n",
    "    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "    df['Duration_x_HR'] = df['Duration'] * df['Heart_Rate']\n",
    "    return df\n",
    "\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "# Define X and log-transformed y\n",
    "X = train_df.drop(columns=['id', 'Calories'])\n",
    "y = np.log1p(train_df['Calories'])  # log1p transformation\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Optuna Tuning for XGBoost ---\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_val), np.expm1(preds)))\n",
    "    return rmsle\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters from Optuna:\", best_params)\n",
    "\n",
    "# Train best XGBoost\n",
    "xgb_model = XGBRegressor(**best_params)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = np.expm1(xgb_model.predict(X_val))\n",
    "\n",
    "# Train LightGBM\n",
    "lgbm_model = LGBMRegressor(random_state=42)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_preds = np.expm1(lgbm_model.predict(X_val))\n",
    "\n",
    "# Train RandomForest\n",
    "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = np.expm1(rf_model.predict(X_val))\n",
    "\n",
    "# Ensemble\n",
    "final_preds = (xgb_preds + lgbm_preds + rf_preds) / 3\n",
    "final_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_val), final_preds))\n",
    "print(f\"Ensemble RMSLE on Validation Set: {final_rmsle:.5f}\")\n",
    "\n",
    "# Retrain all on full data\n",
    "xgb_model.fit(X, y)\n",
    "lgbm_model.fit(X, y)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "xgb_test = np.expm1(xgb_model.predict(X_test))\n",
    "lgbm_test = np.expm1(lgbm_model.predict(X_test))\n",
    "rf_test = np.expm1(rf_model.predict(X_test))\n",
    "final_test_preds = (xgb_test + lgbm_test + rf_test) / 3\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Calories': final_test_preds\n",
    "})\n",
    "submission_file = \"D:\\\\0-projects\\\\1- calorie prediction\\\\submission_ensemble_optuna.csv\"\n",
    "submission.to_csv(submission_file, index=False)\n",
    "print(f\"✅ Final ensemble submission saved to: {submission_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d68bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
